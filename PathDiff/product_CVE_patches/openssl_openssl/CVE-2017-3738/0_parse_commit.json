{
    "current_hash": "e502cc86df9dafded1694fceb3228ee34d11c11a",
    "parent_hash": "6fca9feeffc18472c8153fd0bf8b0a95cc05c504",
    "modified_file_0": {
        "mod_filename": "crypto/bn/asm/rsaz-avx2.pl",
        "status": "modified",
        "add_lines": 7,
        "dele_lines": 8,
        "patch": "@@ -246,7 +246,7 @@\n \tvmovdqu\t\t32*8-128($ap), $ACC8\n \n \tlea\t192(%rsp), $tp0\t\t\t# 64+128=192\n-\tvpbroadcastq\t.Land_mask(%rip), $AND_MASK\n+\tvmovdqu\t.Land_mask(%rip), $AND_MASK\n \tjmp\t.LOOP_GRANDE_SQR_1024\n \n .align\t32\n@@ -1077,10 +1077,10 @@\n \tvpmuludq\t32*6-128($np),$Yi,$TEMP1\n \tvpaddq\t\t$TEMP1,$ACC6,$ACC6\n \tvpmuludq\t32*7-128($np),$Yi,$TEMP2\n-\t vpblendd\t\\$3, $ZERO, $ACC9, $ACC9\t# correct $ACC3\n+\t vpblendd\t\\$3, $ZERO, $ACC9, $TEMP1\t# correct $ACC3\n \tvpaddq\t\t$TEMP2,$ACC7,$ACC7\n \tvpmuludq\t32*8-128($np),$Yi,$TEMP0\n-\t vpaddq\t\t$ACC9, $ACC3, $ACC3\t\t# correct $ACC3\n+\t vpaddq\t\t$TEMP1, $ACC3, $ACC3\t\t# correct $ACC3\n \tvpaddq\t\t$TEMP0,$ACC8,$ACC8\n \n \tmov\t%rbx, %rax\n@@ -1093,7 +1093,9 @@\n \t vmovdqu\t-8+32*2-128($ap),$TEMP2\n \n \tmov\t$r1, %rax\n+\t vpblendd\t\\$0xfc, $ZERO, $ACC9, $ACC9\t# correct $ACC3\n \timull\t$n0, %eax\n+\t vpaddq\t\t$ACC9,$ACC4,$ACC4\t\t# correct $ACC3\n \tand\t\\$0x1fffffff, %eax\n \n \t imulq\t16-128($ap),%rbx\n@@ -1329,15 +1331,12 @@\n #\tBut as we underutilize resources, it's possible to correct in\n #\teach iteration with marginal performance loss. But then, as\n #\twe do it in each iteration, we can correct less digits, and\n-#\tavoid performance penalties completely. Also note that we\n-#\tcorrect only three digits out of four. This works because\n-#\tmost significant digit is subjected to less additions.\n+#\tavoid performance penalties completely.\n \n $TEMP0 = $ACC9;\n $TEMP3 = $Bi;\n $TEMP4 = $Yi;\n $code.=<<___;\n-\tvpermq\t\t\\$0, $AND_MASK, $AND_MASK\n \tvpaddq\t\t(%rsp), $TEMP1, $ACC0\n \n \tvpsrlq\t\t\\$29, $ACC0, $TEMP1\n@@ -1770,7 +1769,7 @@\n \n .align\t64\n .Land_mask:\n-\t.quad\t0x1fffffff,0x1fffffff,0x1fffffff,-1\n+\t.quad\t0x1fffffff,0x1fffffff,0x1fffffff,0x1fffffff\n .Lscatter_permd:\n \t.long\t0,2,4,6,7,7,7,7\n .Lgather_permd:"
    }
}