{
    "current_hash": "e11f55585f614645b360563072ffeb5c3eeff162",
    "parent_hash": "ec98fee0c3b6ea8724542b103795d551afc25b1a",
    "modified_file_0": {
        "mod_filename": "tensorflow/lite/delegates/delegate_test.cc",
        "status": "modified",
        "add_lines": 32,
        "dele_lines": 16,
        "patch": "@@ -42,9 +42,12 @@ TfLiteRegistration AddOpRegistration() {\n \n   reg.prepare = [](TfLiteContext* context, TfLiteNode* node) {\n     // Set output size to input size\n-    const TfLiteTensor* input1 = GetInput(context, node, 0);\n-    const TfLiteTensor* input2 = GetInput(context, node, 1);\n-    TfLiteTensor* output = GetOutput(context, node, 0);\n+    const TfLiteTensor* input1;\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input1));\n+    const TfLiteTensor* input2;\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &input2));\n+    TfLiteTensor* output;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n \n     TF_LITE_ENSURE_EQ(context, input1->dims->size, input2->dims->size);\n     for (int i = 0; i < input1->dims->size; ++i) {\n@@ -58,13 +61,16 @@ TfLiteRegistration AddOpRegistration() {\n \n   reg.invoke = [](TfLiteContext* context, TfLiteNode* node) {\n     // Copy input data to output data.\n-    const TfLiteTensor* a0 = GetInput(context, node, 0);\n+    const TfLiteTensor* a0;\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &a0));\n     TF_LITE_ENSURE(context, a0);\n     TF_LITE_ENSURE(context, a0->data.f);\n-    const TfLiteTensor* a1 = GetInput(context, node, 1);\n+    const TfLiteTensor* a1;\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &a1));\n     TF_LITE_ENSURE(context, a1);\n     TF_LITE_ENSURE(context, a1->data.f);\n-    TfLiteTensor* out = GetOutput(context, node, 0);\n+    TfLiteTensor* out;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &out));\n     TF_LITE_ENSURE(context, out);\n     TF_LITE_ENSURE(context, out->data.f);\n     int num = a0->dims->data[0];\n@@ -267,7 +273,8 @@ class TestDelegate : public ::testing::Test {\n             a0 = GetInput(context, node, 0);\n             a1 = a0;\n           }\n-          TfLiteTensor* out = GetOutput(context, node, 0);\n+          TfLiteTensor* out;\n+          TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &out));\n           int num = 1;\n           for (int i = 0; i < a0->dims->size; ++i) {\n             num *= a0->dims->data[i];\n@@ -289,8 +296,10 @@ class TestDelegate : public ::testing::Test {\n         reg.prepare = [](TfLiteContext* context, TfLiteNode* node) {\n           // Shapes should already by propagated by the runtime, just need to\n           // check.\n-          const TfLiteTensor* input1 = GetInput(context, node, 0);\n-          TfLiteTensor* output = GetOutput(context, node, 0);\n+          const TfLiteTensor* input1;\n+          TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input1));\n+          TfLiteTensor* output;\n+          TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n           const int input_dims_size = input1->dims->size;\n           TF_LITE_ENSURE(context, output->dims->size == input_dims_size);\n           for (int i = 0; i < input_dims_size; ++i) {\n@@ -315,7 +324,8 @@ class TestDelegate : public ::testing::Test {\n             input1 = GetInput(context, node, 0);\n             input2 = input1;\n           }\n-          TfLiteTensor* output = GetOutput(context, node, 0);\n+          TfLiteTensor* output;\n+          TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n \n           TF_LITE_ENSURE_STATUS(context->ResizeTensor(\n               context, output, TfLiteIntArrayCopy(input1->dims)));\n@@ -1169,11 +1179,14 @@ class TestDelegateWithDynamicTensors : public ::testing::Test {\n \n     reg.prepare = [](TfLiteContext* context, TfLiteNode* node) {\n       // Output 0 is dynamic\n-      TfLiteTensor* output0 = GetOutput(context, node, 0);\n+      TfLiteTensor* output0;\n+      TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output0));\n       SetTensorToDynamic(output0);\n       // Output 1 has the same shape as input.\n-      const TfLiteTensor* input = GetInput(context, node, 0);\n-      TfLiteTensor* output1 = GetOutput(context, node, 1);\n+      const TfLiteTensor* input;\n+      TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+      TfLiteTensor* output1;\n+      TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 1, &output1));\n       TF_LITE_ENSURE_STATUS(context->ResizeTensor(\n           context, output1, TfLiteIntArrayCopy(input->dims)));\n       return kTfLiteOk;\n@@ -1193,11 +1206,14 @@ class TestDelegateWithDynamicTensors : public ::testing::Test {\n       // If tensors are resized, the runtime should propagate shapes\n       // automatically if correct flag is set. Ensure values are correct.\n       // Output 0 should be dynamic.\n-      TfLiteTensor* output0 = GetOutput(context, node, 0);\n+      TfLiteTensor* output0;\n+      TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output0));\n       TF_LITE_ENSURE(context, IsDynamicTensor(output0));\n       // Output 1 has the same shape as input.\n-      const TfLiteTensor* input = GetInput(context, node, 0);\n-      TfLiteTensor* output1 = GetOutput(context, node, 1);\n+      const TfLiteTensor* input;\n+      TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+      TfLiteTensor* output1;\n+      TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 1, &output1));\n       TF_LITE_ENSURE(context, input->dims->size == output1->dims->size);\n       TF_LITE_ENSURE(context, input->dims->data[0] == output1->dims->data[0]);\n       return kTfLiteOk;"
    },
    "modified_file_1": {
        "mod_filename": "tensorflow/lite/delegates/gpu/common/model_builder.cc",
        "status": "modified",
        "add_lines": 7,
        "dele_lines": 1,
        "patch": "@@ -1166,6 +1166,9 @@ class MulOperationParser : public TFLiteOperationParser {\n     }\n     auto input0 = tflite::GetInput(context, tflite_node, 0);\n     auto input1 = tflite::GetInput(context, tflite_node, 1);\n+    if (input0 == nullptr || input1 == nullptr) {\n+      return absl::InvalidArgumentError(\"At least one input tensor is null\");\n+    }\n     if (input0->dims->size == input1->dims->size) {\n       // this code checks that at least one input of Mul not smaller in all\n       // dimensions. Sometimes Mul used for matrix-vector multiplication that we\n@@ -1380,7 +1383,10 @@ class PadOperationParser : public TFLiteOperationParser {\n     RETURN_IF_ERROR(CheckInputsOutputs(context, tflite_node,\n                                        /*runtime_inputs=*/1, /*outputs=*/1));\n     RETURN_IF_ERROR(CheckTensorIsAvailable(context, tflite_node, 1));\n-    auto pad_tensor = tflite::GetInput(context, tflite_node, 1);\n+    const TfLiteTensor* pad_tensor = tflite::GetInput(context, tflite_node, 1);\n+    if (pad_tensor == nullptr) {\n+      return absl::InvalidArgumentError(\"Padding tensor was null\");\n+    }\n     if (pad_tensor->dims->size != 2) {\n       return absl::InvalidArgumentError(absl::StrCat(\n           \"Invalid paddings tensor dimension: expected 2 dim, got \","
    },
    "modified_file_2": {
        "mod_filename": "tensorflow/lite/experimental/delegates/coreml/builders/convolution_op_builder.cc",
        "status": "modified",
        "add_lines": 3,
        "dele_lines": 1,
        "patch": "@@ -328,7 +328,9 @@ bool IsConvolutionOpSupported(const TfLiteRegistration* registration,\n   const int kOutputShapeTensor = 0;  // Only used for TransposeConv\n   const int kWeightTensor = 1;\n   const int kBiasTensor = 2;  // Only used for non-TransposeConv\n-  const TfLiteTensor* weights = GetInput(context, node, kWeightTensor);\n+  const TfLiteTensor* weights;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kWeightTensor, &weights));\n   const int max_kernel_size = 16384;\n   if (!IsConstantTensor(weights)) {\n     return false;"
    },
    "modified_file_3": {
        "mod_filename": "tensorflow/lite/experimental/delegates/coreml/builders/fully_connected_op_builder.cc",
        "status": "modified",
        "add_lines": 6,
        "dele_lines": 3,
        "patch": "@@ -153,8 +153,10 @@ bool IsFullyConnectedOpSupported(const TfLiteRegistration* registration,\n   if (fc_params->weights_format != kTfLiteFullyConnectedWeightsFormatDefault) {\n     return false;\n   }\n-  const TfLiteTensor* input = GetInput(context, node, kInput);\n-  const TfLiteTensor* weights = GetInput(context, node, kWeights);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInput, &input));\n+  const TfLiteTensor* weights;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kWeights, &weights));\n \n   if (!IsFloatType(input->type)) {\n     return false;\n@@ -169,7 +171,8 @@ bool IsFullyConnectedOpSupported(const TfLiteRegistration* registration,\n   }\n \n   if (node->inputs->size > 2) {\n-    const TfLiteTensor* bias = GetInput(context, node, kBias);\n+    const TfLiteTensor* bias;\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBias, &bias));\n     if (!IsFloatType(bias->type) || !IsConstantTensor(bias)) {\n       return false;\n     }"
    },
    "modified_file_4": {
        "mod_filename": "tensorflow/lite/experimental/delegates/coreml/builders/pad_op_builder.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 1,
        "patch": "@@ -97,7 +97,8 @@ OpBuilder* CreateMirrorPadOpBuilder(GraphBuilder* graph_builder) {\n bool IsPadOpSupported(const TfLiteRegistration* registration,\n                       const TfLiteNode* node, TfLiteContext* context) {\n   // padding is d x 2 tensor, where d is the dimension of input.\n-  const TfLiteTensor* padding = GetInput(context, node, 1);\n+  const TfLiteTensor* padding;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &padding));\n   if (!IsConstantTensor(padding)) {\n     TF_LITE_KERNEL_LOG(context,\n                        \"%s: Only constant padding is supported for PAD.\","
    },
    "modified_file_5": {
        "mod_filename": "tensorflow/lite/experimental/delegates/coreml/builders/reshape_op_builder.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 1,
        "patch": "@@ -126,7 +126,8 @@ bool IsReshapeOpSupported(const TfLiteRegistration* registration,\n   }\n \n   const int kShapeTensor = 1;\n-  const auto* shape = GetInput(context, node, kShapeTensor);\n+  const TfLiteTensor* shape;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kShapeTensor, &shape));\n   if (shape->allocation_type != kTfLiteMmapRo) {\n     TF_LITE_KERNEL_LOG(context, \"Reshape has non-const shape.\");\n     return false;"
    },
    "modified_file_6": {
        "mod_filename": "tensorflow/lite/experimental/kernels/ctc_beam_search_decoder.cc",
        "status": "modified",
        "add_lines": 34,
        "dele_lines": 15,
        "patch": "@@ -62,14 +62,17 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   // The outputs should be top_paths * 3 + 1.\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 3 * top_paths + 1);\n \n-  const TfLiteTensor* inputs = GetInput(context, node, kInputsTensor);\n+  const TfLiteTensor* inputs;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputsTensor, &inputs));\n   TF_LITE_ENSURE_EQ(context, NumDimensions(inputs), 3);\n   // TensorFlow only supports float.\n   TF_LITE_ENSURE_EQ(context, inputs->type, kTfLiteFloat32);\n   const int batch_size = SizeOfDimension(inputs, 1);\n \n-  const TfLiteTensor* sequence_length =\n-      GetInput(context, node, kSequenceLengthTensor);\n+  const TfLiteTensor* sequence_length;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kSequenceLengthTensor,\n+                                          &sequence_length));\n   TF_LITE_ENSURE_EQ(context, NumDimensions(sequence_length), 1);\n   TF_LITE_ENSURE_EQ(context, NumElements(sequence_length), batch_size);\n   // TensorFlow only supports int32.\n@@ -78,17 +81,23 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   // Resize decoded outputs.\n   // Do not resize indices & values cause we don't know the values yet.\n   for (int i = 0; i < top_paths; ++i) {\n-    TfLiteTensor* indices = GetOutput(context, node, i);\n+    TfLiteTensor* indices;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, i, &indices));\n     SetTensorToDynamic(indices);\n-    TfLiteTensor* values = GetOutput(context, node, i + top_paths);\n+    TfLiteTensor* values;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetOutputSafe(context, node, i + top_paths, &values));\n     SetTensorToDynamic(values);\n-    TfLiteTensor* output_shape = GetOutput(context, node, i + 2 * top_paths);\n+    TfLiteTensor* output_shape;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, i + 2 * top_paths,\n+                                             &output_shape));\n     SetTensorToDynamic(output_shape);\n   }\n \n   // Resize log probability outputs.\n-  TfLiteTensor* log_probability_output =\n-      GetOutput(context, node, top_paths * 3);\n+  TfLiteTensor* log_probability_output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, top_paths * 3,\n+                                           &log_probability_output));\n   TfLiteIntArray* log_probability_output_shape_array = TfLiteIntArrayCreate(2);\n   log_probability_output_shape_array->data[0] = batch_size;\n   log_probability_output_shape_array->data[1] = top_paths;\n@@ -127,13 +136,18 @@ TfLiteStatus StoreAllDecodedSequences(\n     const int32_t p_num = num_entries[p];\n \n     // Resize the decoded outputs.\n-    TfLiteTensor* indices = GetOutput(context, node, p);\n+    TfLiteTensor* indices;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, p, &indices));\n     TF_LITE_ENSURE_OK(context, Resize(context, {p_num, 2}, indices));\n \n-    TfLiteTensor* values = GetOutput(context, node, p + top_paths);\n+    TfLiteTensor* values;\n+    TF_LITE_ENSURE_OK(context,\n+                      GetOutputSafe(context, node, p + top_paths, &values));\n     TF_LITE_ENSURE_OK(context, Resize(context, {p_num}, values));\n \n-    TfLiteTensor* decoded_shape = GetOutput(context, node, p + 2 * top_paths);\n+    TfLiteTensor* decoded_shape;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, p + 2 * top_paths,\n+                                             &decoded_shape));\n     TF_LITE_ENSURE_OK(context, Resize(context, {2}, decoded_shape));\n \n     int32_t max_decoded = 0;\n@@ -161,9 +175,12 @@ TfLiteStatus StoreAllDecodedSequences(\n }\n \n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* inputs = GetInput(context, node, kInputsTensor);\n-  const TfLiteTensor* sequence_length =\n-      GetInput(context, node, kSequenceLengthTensor);\n+  const TfLiteTensor* inputs;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputsTensor, &inputs));\n+  const TfLiteTensor* sequence_length;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kSequenceLengthTensor,\n+                                          &sequence_length));\n   const CTCBeamSearchDecoderParams* option =\n       reinterpret_cast<CTCBeamSearchDecoderParams*>(node->user_data);\n \n@@ -207,7 +224,9 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   std::vector<std::vector<std::vector<int>>> best_paths(batch_size);\n   std::vector<float> log_probs;\n \n-  TfLiteTensor* log_probabilities = GetOutput(context, node, 3 * top_paths);\n+  TfLiteTensor* log_probabilities;\n+  TF_LITE_ENSURE_OK(\n+      context, GetOutputSafe(context, node, 3 * top_paths, &log_probabilities));\n   float* log_probabilities_output = GetTensorData<float>(log_probabilities);\n \n   // Assumption: the blank index is num_classes - 1"
    },
    "modified_file_7": {
        "mod_filename": "tensorflow/lite/experimental/kernels/unidirectional_sequence_gru.cc",
        "status": "modified",
        "add_lines": 54,
        "dele_lines": 22,
        "patch": "@@ -127,44 +127,55 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, node->outputs->size, kOutputNum);\n \n   // input's dim = [n_time, n_batch, n_input]\n-  const TfLiteTensor* input = GetInput(context, node, kInput);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInput, &input));\n   TF_LITE_ENSURE_EQ(context, input->dims->size, 3);\n   const int n_time = input->dims->data[0];\n   const int n_batch = input->dims->data[1];\n   const int n_input = input->dims->data[2];\n \n   // input_state's dim = [n_batch, n_output]\n-  const TfLiteTensor* input_state = GetInput(context, node, kInputState);\n+  const TfLiteTensor* input_state;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputState, &input_state));\n   TF_LITE_ENSURE_EQ(context, input_state->dims->size, 2);\n   TF_LITE_ENSURE_EQ(context, input_state->dims->data[0], n_batch);\n   const int n_output = input_state->dims->data[1];\n \n   // gate_weight' dim = [2 * n_output, n_input + n_output]\n-  const TfLiteTensor* gate_weight = GetInput(context, node, kGateWeight);\n+  const TfLiteTensor* gate_weight;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kGateWeight, &gate_weight));\n   TF_LITE_ENSURE_EQ(context, gate_weight->dims->size, 2);\n   TF_LITE_ENSURE_EQ(context, gate_weight->dims->data[0], 2 * n_output);\n   TF_LITE_ENSURE_EQ(context, gate_weight->dims->data[1], n_input + n_output);\n \n   // gate_bias' dim = [2 * n_output]\n-  const TfLiteTensor* gate_bias = GetInput(context, node, kGateBias);\n+  const TfLiteTensor* gate_bias;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kGateBias, &gate_bias));\n   TF_LITE_ENSURE_EQ(context, gate_bias->dims->size, 1);\n   TF_LITE_ENSURE_EQ(context, gate_bias->dims->data[0], 2 * n_output);\n \n   // candidate_weight' dim = [n_output, n_input + n_output]\n-  const TfLiteTensor* candidate_weight =\n-      GetInput(context, node, kCandidateWeight);\n+  const TfLiteTensor* candidate_weight;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kCandidateWeight,\n+                                          &candidate_weight));\n   TF_LITE_ENSURE_EQ(context, candidate_weight->dims->size, 2);\n   TF_LITE_ENSURE_EQ(context, candidate_weight->dims->data[0], n_output);\n   TF_LITE_ENSURE_EQ(context, candidate_weight->dims->data[1],\n                     n_input + n_output);\n \n   // candidate_bias' dim = [n_output]\n-  const TfLiteTensor* candidate_bias = GetInput(context, node, kCandidateBias);\n+  const TfLiteTensor* candidate_bias;\n+  TF_LITE_ENSURE_OK(\n+      context, GetInputSafe(context, node, kCandidateBias, &candidate_bias));\n   TF_LITE_ENSURE_EQ(context, candidate_bias->dims->size, 1);\n   TF_LITE_ENSURE_EQ(context, candidate_bias->dims->data[0], n_output);\n \n   // output's dim = [n_time, n_batch, n_output]\n-  TfLiteTensor* output = GetOutput(context, node, kOutput);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, kOutput, &output));\n   TfLiteIntArray* output_size = TfLiteIntArrayCreate(3);\n   output_size->data[0] = n_time;\n   output_size->data[1] = n_batch;\n@@ -173,7 +184,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n                     context->ResizeTensor(context, output, output_size));\n \n   // output_state's dim = [n_batch, n_output]\n-  TfLiteTensor* output_state = GetOutput(context, node, kOutputState);\n+  TfLiteTensor* output_state;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputState, &output_state));\n   TF_LITE_ENSURE_OK(\n       context, context->ResizeTensor(context, output_state,\n                                      TfLiteIntArrayCopy(input_state->dims)));\n@@ -183,7 +196,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n \n   // activation's dim = [n_batch, 2 * n_output]\n   node->temporaries->data[kActivation] = *scratch_tensor_index;\n-  TfLiteTensor* activation = GetTemporary(context, node, kActivation);\n+  TfLiteTensor* activation;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetTemporarySafe(context, node, kActivation, &activation));\n   activation->type = input->type;\n   activation->allocation_type = kTfLiteArenaRw;\n   TfLiteIntArray* activation_size = TfLiteIntArrayCreate(2);\n@@ -194,7 +209,8 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n \n   // concat's dim  = [n_batch, n_input + n_output]\n   node->temporaries->data[kConcat] = (*scratch_tensor_index) + kConcat;\n-  TfLiteTensor* concat = GetTemporary(context, node, kConcat);\n+  TfLiteTensor* concat;\n+  TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kConcat, &concat));\n   concat->type = input->type;\n   concat->allocation_type = kTfLiteArenaRw;\n   TfLiteIntArray* concat_size = TfLiteIntArrayCreate(2);\n@@ -207,17 +223,33 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = GetInput(context, node, kInput);\n-  const TfLiteTensor* input_state = GetInput(context, node, kInputState);\n-  const TfLiteTensor* gate_weight = GetInput(context, node, kGateWeight);\n-  const TfLiteTensor* gate_bias = GetInput(context, node, kGateBias);\n-  const TfLiteTensor* candidate_weight =\n-      GetInput(context, node, kCandidateWeight);\n-  const TfLiteTensor* candidate_bias = GetInput(context, node, kCandidateBias);\n-  TfLiteTensor* output = GetOutput(context, node, kOutput);\n-  TfLiteTensor* output_state = GetOutput(context, node, kOutputState);\n-  TfLiteTensor* activation = GetTemporary(context, node, kActivation);\n-  TfLiteTensor* concat = GetTemporary(context, node, kConcat);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInput, &input));\n+  const TfLiteTensor* input_state;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kInputState, &input_state));\n+  const TfLiteTensor* gate_weight;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kGateWeight, &gate_weight));\n+  const TfLiteTensor* gate_bias;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kGateBias, &gate_bias));\n+  const TfLiteTensor* candidate_weight;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kCandidateWeight,\n+                                          &candidate_weight));\n+  const TfLiteTensor* candidate_bias;\n+  TF_LITE_ENSURE_OK(\n+      context, GetInputSafe(context, node, kCandidateBias, &candidate_bias));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, kOutput, &output));\n+  TfLiteTensor* output_state;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputState, &output_state));\n+  TfLiteTensor* activation;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetTemporarySafe(context, node, kActivation, &activation));\n+  TfLiteTensor* concat;\n+  TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kConcat, &concat));\n   auto cpu_backend_context = CpuBackendContext::GetFromContext(context);\n \n   if (gate_weight->type == kTfLiteFloat32) {"
    },
    "modified_file_8": {
        "mod_filename": "tensorflow/lite/experimental/microfrontend/audio_microfrontend.cc",
        "status": "modified",
        "add_lines": 10,
        "dele_lines": 4,
        "patch": "@@ -91,8 +91,11 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n \n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n-  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputTensor, &output));\n \n   TF_LITE_ENSURE_EQ(context, NumDimensions(input), 1);\n \n@@ -180,8 +183,11 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n       reinterpret_cast<TfLiteAudioMicrofrontendParams*>(node->user_data);\n   FrontendReset(data->state);\n \n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n-  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kOutputTensor, &output));\n \n   if (data->out_float) {\n     GenerateFeatures<float>(data, input, output);"
    },
    "modified_file_9": {
        "mod_filename": "tensorflow/lite/interpreter_test.cc",
        "status": "modified",
        "add_lines": 40,
        "dele_lines": 20,
        "patch": "@@ -621,8 +621,10 @@ TfLiteRegistration GetPassthroughOpRegistration() {\n   reg.prepare = [](TfLiteContext* context, TfLiteNode* node) {\n     auto* first_new_tensor = static_cast<int*>(node->user_data);\n \n-    const TfLiteTensor* tensor0 = GetInput(context, node, 0);\n-    TfLiteTensor* tensor1 = GetOutput(context, node, 0);\n+    const TfLiteTensor* tensor0;\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &tensor0));\n+    TfLiteTensor* tensor1;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &tensor1));\n \n     TfLiteIntArray* newSize = TfLiteIntArrayCopy(tensor0->dims);\n     TF_LITE_ENSURE_STATUS(context->ResizeTensor(context, tensor1, newSize));\n@@ -646,7 +648,8 @@ TfLiteRegistration GetPassthroughOpRegistration() {\n     return kTfLiteOk;\n   };\n   reg.invoke = [](TfLiteContext* context, TfLiteNode* node) {\n-    const TfLiteTensor* a0 = GetInput(context, node, 0);\n+    const TfLiteTensor* a0;\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &a0));\n \n     auto populate = [&](int id) {\n       TfLiteTensor* t = &context->tensors[id];\n@@ -780,8 +783,10 @@ TEST(BasicInterpreter, ThreeStepAllocate) {\n   // String-in String-out node.\n   TfLiteRegistration reg_copy = {nullptr, nullptr, nullptr, nullptr};\n   reg_copy.invoke = [](TfLiteContext* context, TfLiteNode* node) {\n-    const TfLiteTensor* input = GetInput(context, node, 0);\n-    TfLiteTensor* output = GetOutput(context, node, 0);\n+    const TfLiteTensor* input;\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n+    TfLiteTensor* output;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n     DynamicBuffer buf;\n     StringRef str_ref = GetString(input, 0);\n     buf.AddString(str_ref);\n@@ -792,14 +797,17 @@ TEST(BasicInterpreter, ThreeStepAllocate) {\n   // String-in Int-out node.\n   TfLiteRegistration reg_len = {nullptr, nullptr, nullptr, nullptr};\n   reg_len.prepare = [](TfLiteContext* context, TfLiteNode* node) {\n-    TfLiteTensor* output = GetOutput(context, node, 0);\n+    TfLiteTensor* output;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n     TfLiteIntArray* outputSize = TfLiteIntArrayCreate(1);\n     outputSize->data[0] = 1;\n     return context->ResizeTensor(context, output, outputSize);\n   };\n   reg_len.invoke = [](TfLiteContext* context, TfLiteNode* node) {\n-    const TfLiteTensor* a0 = GetInput(context, node, 0);\n-    TfLiteTensor* a1 = GetOutput(context, node, 0);\n+    const TfLiteTensor* a0;\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &a0));\n+    TfLiteTensor* a1;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &a1));\n     a1->data.i32[0] = a0->bytes;\n     return kTfLiteOk;\n   };\n@@ -848,14 +856,18 @@ TEST(BasicInterpreter, AllocateTwice) {\n \n   TfLiteRegistration reg = {nullptr, nullptr, nullptr, nullptr};\n   reg.prepare = [](TfLiteContext* context, TfLiteNode* node) {\n-    const TfLiteTensor* tensor0 = GetInput(context, node, 0);\n-    TfLiteTensor* tensor1 = GetOutput(context, node, 0);\n+    const TfLiteTensor* tensor0;\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &tensor0));\n+    TfLiteTensor* tensor1;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &tensor1));\n     TfLiteIntArray* newSize = TfLiteIntArrayCopy(tensor0->dims);\n     return context->ResizeTensor(context, tensor1, newSize);\n   };\n   reg.invoke = [](TfLiteContext* context, TfLiteNode* node) {\n-    const TfLiteTensor* a0 = GetInput(context, node, 0);\n-    TfLiteTensor* a1 = GetOutput(context, node, 0);\n+    const TfLiteTensor* a0;\n+    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &a0));\n+    TfLiteTensor* a1;\n+    TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &a1));\n     int num = a0->dims->data[0];\n     for (int i = 0; i < num; i++) {\n       a1->data.f[i] = a0->data.f[i];\n@@ -1205,8 +1217,10 @@ class TestExecutionPlan : public ::testing::Test {\n \n     reg.prepare = [](TfLiteContext* context, TfLiteNode* node) {\n       // Set output size to input size\n-      const TfLiteTensor* tensor0 = GetInput(context, node, 0);\n-      TfLiteTensor* tensor1 = GetOutput(context, node, 0);\n+      const TfLiteTensor* tensor0;\n+      TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &tensor0));\n+      TfLiteTensor* tensor1;\n+      TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &tensor1));\n       TfLiteIntArray* newSize = TfLiteIntArrayCopy(tensor0->dims);\n       return context->ResizeTensor(context, tensor1, newSize);\n     };\n@@ -1215,8 +1229,10 @@ class TestExecutionPlan : public ::testing::Test {\n       CallReporting* call_reporting =\n           static_cast<CallReporting*>(node->builtin_data);\n       // Copy input data to output data.\n-      const TfLiteTensor* a0 = GetInput(context, node, 0);\n-      TfLiteTensor* a1 = GetOutput(context, node, 0);\n+      const TfLiteTensor* a0;\n+      TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &a0));\n+      TfLiteTensor* a1;\n+      TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &a1));\n       int num = a0->dims->data[0];\n       for (int i = 0; i < num; i++) {\n         a1->data.f[i] = a0->data.f[i];\n@@ -1403,8 +1419,10 @@ class CancellationTest : public ::testing::Test {\n     // Set output size to the input size in CancelOp::Prepare(). Code exists to\n     // have a framework in Prepare. The input and output tensors are not used.\n     reg.prepare = [](TfLiteContext* context, TfLiteNode* node) {\n-      const TfLiteTensor* in_tensor = GetInput(context, node, 0);\n-      TfLiteTensor* out_tensor = GetOutput(context, node, 0);\n+      const TfLiteTensor* in_tensor;\n+      TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &in_tensor));\n+      TfLiteTensor* out_tensor;\n+      TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &out_tensor));\n       TfLiteIntArray* new_size = TfLiteIntArrayCopy(in_tensor->dims);\n       return context->ResizeTensor(context, out_tensor, new_size);\n     };\n@@ -1423,8 +1441,10 @@ class CancellationTest : public ::testing::Test {\n     // Set output size to the input size in OkOp::Prepare(). Code exists to have\n     // a framework in Prepare. The input and output tensors are not used.\n     reg.prepare = [](TfLiteContext* context, TfLiteNode* node) {\n-      const TfLiteTensor* in_tensor = GetInput(context, node, 0);\n-      TfLiteTensor* out_tensor = GetOutput(context, node, 0);\n+      const TfLiteTensor* in_tensor;\n+      TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &in_tensor));\n+      TfLiteTensor* out_tensor;\n+      TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &out_tensor));\n       TfLiteIntArray* new_size = TfLiteIntArrayCopy(in_tensor->dims);\n       return context->ResizeTensor(context, out_tensor, new_size);\n     };"
    },
    "modified_file_10": {
        "mod_filename": "tensorflow/lite/java/src/test/native/interpreter_test_jni.cc",
        "status": "modified",
        "add_lines": 9,
        "dele_lines": 3,
        "patch": "@@ -33,15 +33,21 @@ Java_org_tensorflow_lite_InterpreterTest_getNativeHandleForDelegate(\n       .free = nullptr,\n       .prepare =\n           [](TfLiteContext* context, TfLiteNode* node) {\n-            const TfLiteTensor* input = tflite::GetInput(context, node, 0);\n-            TfLiteTensor* output = tflite::GetOutput(context, node, 0);\n+            const TfLiteTensor* input;\n+            TF_LITE_ENSURE_OK(context,\n+                              tflite::GetInputSafe(context, node, 0, &input));\n+            TfLiteTensor* output;\n+            TF_LITE_ENSURE_OK(context,\n+                              tflite::GetOutputSafe(context, node, 0, &output));\n             TfLiteIntArray* output_dims = TfLiteIntArrayCopy(input->dims);\n             output->type = kTfLiteFloat32;\n             return context->ResizeTensor(context, output, output_dims);\n           },\n       .invoke =\n           [](TfLiteContext* context, TfLiteNode* node) {\n-            TfLiteTensor* output = tflite::GetOutput(context, node, 0);\n+            TfLiteTensor* output;\n+            TF_LITE_ENSURE_OK(context,\n+                              tflite::GetOutputSafe(context, node, 0, &output));\n             std::fill(output->data.f,\n                       output->data.f + tflite::NumElements(output), 7.0f);\n             return kTfLiteOk;"
    },
    "modified_file_11": {
        "mod_filename": "tensorflow/lite/kernels/hashtable/hashtable.cc",
        "status": "modified",
        "add_lines": 6,
        "dele_lines": 5,
        "patch": "@@ -80,9 +80,9 @@ TfLiteStatus PrepareHashtable(TfLiteContext* context, TfLiteNode* node) {\n                               (params->key_dtype == kTfLiteString &&\n                                params->value_dtype == kTfLiteInt64));\n \n-  TfLiteTensor* resource_handle_tensor =\n-      GetOutput(context, node, kResourceHandleTensor);\n-  TF_LITE_ENSURE(context, resource_handle_tensor != nullptr);\n+  TfLiteTensor* resource_handle_tensor;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, kResourceHandleTensor,\n+                                           &resource_handle_tensor));\n   TF_LITE_ENSURE_EQ(context, resource_handle_tensor->type, kTfLiteInt32);\n   TfLiteIntArray* outputSize = TfLiteIntArrayCreate(1);\n   outputSize->data[0] = 1;\n@@ -97,8 +97,9 @@ TfLiteStatus EvalHashtable(TfLiteContext* context, TfLiteNode* node) {\n   // The resource id is generated based on the given table name.\n   const int resource_id = std::hash<std::string>{}(params->table_name);\n \n-  TfLiteTensor* resource_handle_tensor =\n-      GetOutput(context, node, kResourceHandleTensor);\n+  TfLiteTensor* resource_handle_tensor;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, kResourceHandleTensor,\n+                                           &resource_handle_tensor));\n   auto* resource_handle_data =\n       GetTensorData<std::int32_t>(resource_handle_tensor);\n   resource_handle_data[0] = resource_id;"
    },
    "modified_file_12": {
        "mod_filename": "tensorflow/lite/kernels/hashtable/hashtable_find.cc",
        "status": "modified",
        "add_lines": 23,
        "dele_lines": 12,
        "patch": "@@ -34,17 +34,23 @@ TfLiteStatus PrepareHashtableFind(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 3);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n \n-  const TfLiteTensor* input_resource_id_tensor =\n-      GetInput(context, node, kInputResourceIdTensor);\n+  const TfLiteTensor* input_resource_id_tensor;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputResourceIdTensor,\n+                                          &input_resource_id_tensor));\n   TF_LITE_ENSURE_EQ(context, input_resource_id_tensor->type, kTfLiteInt32);\n   TF_LITE_ENSURE_EQ(context, NumDimensions(input_resource_id_tensor), 1);\n   TF_LITE_ENSURE_EQ(context, SizeOfDimension(input_resource_id_tensor, 0), 1);\n \n-  const TfLiteTensor* default_value_tensor =\n-      GetInput(context, node, kDefaultValueTensor);\n+  const TfLiteTensor* default_value_tensor;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kDefaultValueTensor,\n+                                          &default_value_tensor));\n \n-  const TfLiteTensor* key_tensor = GetInput(context, node, kKeyTensor);\n-  TfLiteTensor* output_tensor = GetOutput(context, node, kOutputTensor);\n+  const TfLiteTensor* key_tensor;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kKeyTensor, &key_tensor));\n+  TfLiteTensor* output_tensor;\n+  TF_LITE_ENSURE_OK(\n+      context, GetOutputSafe(context, node, kOutputTensor, &output_tensor));\n   TF_LITE_ENSURE_EQ(context, default_value_tensor->type, output_tensor->type);\n   TF_LITE_ENSURE(context, (key_tensor->type == kTfLiteInt64 &&\n                            output_tensor->type == kTfLiteString) ||\n@@ -55,14 +61,19 @@ TfLiteStatus PrepareHashtableFind(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus EvalHashtableFind(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input_resource_id_tensor =\n-      GetInput(context, node, kInputResourceIdTensor);\n+  const TfLiteTensor* input_resource_id_tensor;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputResourceIdTensor,\n+                                          &input_resource_id_tensor));\n   int resource_id = input_resource_id_tensor->data.i32[0];\n \n-  const TfLiteTensor* key_tensor = GetInput(context, node, kKeyTensor);\n-  const TfLiteTensor* default_value_tensor =\n-      GetInput(context, node, kDefaultValueTensor);\n-  TfLiteTensor* output_tensor = GetOutput(context, node, 0);\n+  const TfLiteTensor* key_tensor;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kKeyTensor, &key_tensor));\n+  const TfLiteTensor* default_value_tensor;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kDefaultValueTensor,\n+                                          &default_value_tensor));\n+  TfLiteTensor* output_tensor;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output_tensor));\n \n   Subgraph* subgraph = reinterpret_cast<Subgraph*>(context->impl_);\n   auto& resources = subgraph->resources();"
    },
    "modified_file_13": {
        "mod_filename": "tensorflow/lite/kernels/hashtable/hashtable_import.cc",
        "status": "modified",
        "add_lines": 18,
        "dele_lines": 8,
        "patch": "@@ -33,14 +33,19 @@ TfLiteStatus PrepareHashtableImport(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 3);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 0);\n \n-  const TfLiteTensor* input_resource_id_tensor =\n-      GetInput(context, node, kInputResourceIdTensor);\n+  const TfLiteTensor* input_resource_id_tensor;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputResourceIdTensor,\n+                                          &input_resource_id_tensor));\n   TF_LITE_ENSURE_EQ(context, input_resource_id_tensor->type, kTfLiteInt32);\n   TF_LITE_ENSURE_EQ(context, NumDimensions(input_resource_id_tensor), 1);\n   TF_LITE_ENSURE_EQ(context, SizeOfDimension(input_resource_id_tensor, 0), 1);\n \n-  const TfLiteTensor* key_tensor = GetInput(context, node, kKeyTensor);\n-  const TfLiteTensor* value_tensor = GetInput(context, node, kValueTensor);\n+  const TfLiteTensor* key_tensor;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kKeyTensor, &key_tensor));\n+  const TfLiteTensor* value_tensor;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kValueTensor, &value_tensor));\n   TF_LITE_ENSURE(context, (key_tensor->type == kTfLiteInt64 &&\n                            value_tensor->type == kTfLiteString) ||\n                               (key_tensor->type == kTfLiteString &&\n@@ -52,12 +57,17 @@ TfLiteStatus PrepareHashtableImport(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus EvalHashtableImport(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input_resource_id_tensor =\n-      GetInput(context, node, kInputResourceIdTensor);\n+  const TfLiteTensor* input_resource_id_tensor;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputResourceIdTensor,\n+                                          &input_resource_id_tensor));\n   const int resource_id = input_resource_id_tensor->data.i32[0];\n \n-  const TfLiteTensor* key_tensor = GetInput(context, node, kKeyTensor);\n-  const TfLiteTensor* value_tensor = GetInput(context, node, kValueTensor);\n+  const TfLiteTensor* key_tensor;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kKeyTensor, &key_tensor));\n+  const TfLiteTensor* value_tensor;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetInputSafe(context, node, kValueTensor, &value_tensor));\n \n   Subgraph* subgraph = reinterpret_cast<Subgraph*>(context->impl_);\n   auto& resources = subgraph->resources();"
    },
    "modified_file_14": {
        "mod_filename": "tensorflow/lite/kernels/hashtable/hashtable_size.cc",
        "status": "modified",
        "add_lines": 12,
        "dele_lines": 7,
        "patch": "@@ -32,26 +32,31 @@ TfLiteStatus PrepareHashtableSize(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n \n-  const TfLiteTensor* input_resource_id_tensor =\n-      GetInput(context, node, kInputResourceIdTensor);\n+  const TfLiteTensor* input_resource_id_tensor;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputResourceIdTensor,\n+                                          &input_resource_id_tensor));\n   TF_LITE_ENSURE_EQ(context, input_resource_id_tensor->type, kTfLiteInt32);\n   TF_LITE_ENSURE_EQ(context, NumDimensions(input_resource_id_tensor), 1);\n   TF_LITE_ENSURE_EQ(context, SizeOfDimension(input_resource_id_tensor, 0), 1);\n \n-  TfLiteTensor* output_tensor = GetOutput(context, node, kOutputTensor);\n-  TF_LITE_ENSURE(context, output_tensor != nullptr);\n+  TfLiteTensor* output_tensor;\n+  TF_LITE_ENSURE_OK(\n+      context, GetOutputSafe(context, node, kOutputTensor, &output_tensor));\n   TF_LITE_ENSURE_EQ(context, output_tensor->type, kTfLiteInt64);\n   TfLiteIntArray* outputSize = TfLiteIntArrayCreate(1);\n   outputSize->data[0] = 1;\n   return context->ResizeTensor(context, output_tensor, outputSize);\n }\n \n TfLiteStatus EvalHashtableSize(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input_resource_id_tensor =\n-      GetInput(context, node, kInputResourceIdTensor);\n+  const TfLiteTensor* input_resource_id_tensor;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputResourceIdTensor,\n+                                          &input_resource_id_tensor));\n   int resource_id = input_resource_id_tensor->data.i32[0];\n \n-  TfLiteTensor* output_tensor = GetOutput(context, node, kOutputTensor);\n+  TfLiteTensor* output_tensor;\n+  TF_LITE_ENSURE_OK(\n+      context, GetOutputSafe(context, node, kOutputTensor, &output_tensor));\n   auto* output_data = GetTensorData<std::int64_t>(output_tensor);\n \n   Subgraph* subgraph = reinterpret_cast<Subgraph*>(context->impl_);"
    },
    "modified_file_15": {
        "mod_filename": "tensorflow/lite/kernels/kernel_util.cc",
        "status": "modified",
        "add_lines": 125,
        "dele_lines": 20,
        "patch": "@@ -30,19 +30,70 @@ namespace tflite {\n \n namespace {\n \n-inline TfLiteTensor* GetMutableInput(const TfLiteContext* context,\n-                                     const TfLiteNode* node, int index) {\n-  if (index >= 0 && index < node->inputs->size) {\n-    const int tensor_index = node->inputs->data[index];\n+// Assumes tensor_index is a valid index (in bounds)\n+inline TfLiteTensor* GetTensorAtIndex(const TfLiteContext* context,\n+                                      int tensor_index) {\n+  if (context->tensors != nullptr) {\n+    return &context->tensors[tensor_index];\n+  } else {\n+    return context->GetTensor(context, tensor_index);\n+  }\n+}\n+\n+// Validate in a single place to reduce binary size\n+inline TfLiteStatus ValidateTensorIndexingSafe(const TfLiteContext* context,\n+                                               int index, int max_size,\n+                                               const int* tensor_indices,\n+                                               int* tensor_index) {\n+  if (index < 0 || index >= max_size) {\n+    TF_LITE_KERNEL_LOG(const_cast<TfLiteContext*>(context),\n+                       \"Invalid tensor index %d (not in [0, %d))\\n\", index,\n+                       max_size);\n+    return kTfLiteError;\n+  }\n+  if (tensor_indices[index] == kTfLiteOptionalTensor) {\n+    TF_LITE_KERNEL_LOG(const_cast<TfLiteContext*>(context),\n+                       \"Tensor at index %d was optional but was expected\\n\",\n+                       index);\n+    return kTfLiteError;\n+  }\n+\n+  *tensor_index = tensor_indices[index];\n+  return kTfLiteOk;\n+}\n+\n+// Same as above but returns -1 for invalid inputs instead of status + logging\n+// error.\n+inline int ValidateTensorIndexing(const TfLiteContext* context, int index,\n+                                  int max_size, const int* tensor_indices) {\n+  if (index >= 0 && index < max_size) {\n+    const int tensor_index = tensor_indices[index];\n     if (tensor_index != kTfLiteOptionalTensor) {\n-      if (context->tensors != nullptr) {\n-        return &context->tensors[tensor_index];\n-      } else {\n-        return context->GetTensor(context, tensor_index);\n-      }\n+      return tensor_index;\n     }\n   }\n-  return nullptr;\n+  return -1;\n+}\n+\n+inline TfLiteTensor* GetMutableInput(const TfLiteContext* context,\n+                                     const TfLiteNode* node, int index) {\n+  const int tensor_index = ValidateTensorIndexing(\n+      context, index, node->inputs->size, node->inputs->data);\n+  if (tensor_index < 0) {\n+    return nullptr;\n+  }\n+  return GetTensorAtIndex(context, tensor_index);\n+}\n+\n+inline TfLiteStatus GetMutableInputSafe(const TfLiteContext* context,\n+                                        const TfLiteNode* node, int index,\n+                                        const TfLiteTensor** tensor) {\n+  int tensor_index;\n+  TF_LITE_ENSURE_OK(\n+      context, ValidateTensorIndexingSafe(context, index, node->inputs->size,\n+                                          node->inputs->data, &tensor_index));\n+  *tensor = GetTensorAtIndex(context, tensor_index);\n+  return kTfLiteOk;\n }\n \n }  // anonymous namespace.\n@@ -52,6 +103,11 @@ const TfLiteTensor* GetInput(const TfLiteContext* context,\n   return GetMutableInput(context, node, index);\n }\n \n+TfLiteStatus GetInputSafe(const TfLiteContext* context, const TfLiteNode* node,\n+                          int index, const TfLiteTensor** tensor) {\n+  return GetMutableInputSafe(context, node, index, tensor);\n+}\n+\n TfLiteTensor* GetVariableInput(TfLiteContext* context, const TfLiteNode* node,\n                                int index) {\n   TfLiteTensor* tensor = GetMutableInput(context, node, index);\n@@ -60,24 +116,73 @@ TfLiteTensor* GetVariableInput(TfLiteContext* context, const TfLiteNode* node,\n \n TfLiteTensor* GetOutput(TfLiteContext* context, const TfLiteNode* node,\n                         int index) {\n-  if (index >= 0 && index < node->outputs->size) {\n-    const int tensor_index = node->outputs->data[index];\n-    if (tensor_index != kTfLiteOptionalTensor) {\n-      if (context->tensors != nullptr) {\n-        return &context->tensors[tensor_index];\n-      } else {\n-        return context->GetTensor(context, tensor_index);\n-      }\n-    }\n+  const int tensor_index = ValidateTensorIndexing(\n+      context, index, node->outputs->size, node->outputs->data);\n+  if (tensor_index < 0) {\n+    return nullptr;\n   }\n-  return nullptr;\n+  return GetTensorAtIndex(context, tensor_index);\n+}\n+\n+TfLiteStatus GetOutputSafe(const TfLiteContext* context, const TfLiteNode* node,\n+                           int index, TfLiteTensor** tensor) {\n+  int tensor_index;\n+  TF_LITE_ENSURE_OK(\n+      context, ValidateTensorIndexingSafe(context, index, node->outputs->size,\n+                                          node->outputs->data, &tensor_index));\n+  *tensor = GetTensorAtIndex(context, tensor_index);\n+  return kTfLiteOk;\n }\n \n const TfLiteTensor* GetOptionalInputTensor(const TfLiteContext* context,\n                                            const TfLiteNode* node, int index) {\n   return GetInput(context, node, index);\n }\n \n+#ifndef TF_LITE_STATIC_MEMORY\n+TfLiteTensor* GetTemporary(TfLiteContext* context, const TfLiteNode* node,\n+                           int index) {\n+  const int tensor_index = ValidateTensorIndexing(\n+      context, index, node->temporaries->size, node->temporaries->data);\n+  if (tensor_index < 0) {\n+    return nullptr;\n+  }\n+  return GetTensorAtIndex(context, tensor_index);\n+}\n+\n+TfLiteStatus GetTemporarySafe(const TfLiteContext* context,\n+                              const TfLiteNode* node, int index,\n+                              TfLiteTensor** tensor) {\n+  int tensor_index;\n+  TF_LITE_ENSURE_OK(context, ValidateTensorIndexingSafe(\n+                                 context, index, node->temporaries->size,\n+                                 node->temporaries->data, &tensor_index));\n+  *tensor = GetTensorAtIndex(context, tensor_index);\n+  return kTfLiteOk;\n+}\n+\n+const TfLiteTensor* GetIntermediates(TfLiteContext* context,\n+                                     const TfLiteNode* node, int index) {\n+  const int tensor_index = ValidateTensorIndexing(\n+      context, index, node->intermediates->size, node->intermediates->data);\n+  if (tensor_index < 0) {\n+    return nullptr;\n+  }\n+  return GetTensorAtIndex(context, tensor_index);\n+}\n+\n+TfLiteStatus GetIntermediatesSafe(const TfLiteContext* context,\n+                                  const TfLiteNode* node, int index,\n+                                  TfLiteTensor** tensor) {\n+  int tensor_index;\n+  TF_LITE_ENSURE_OK(context, ValidateTensorIndexingSafe(\n+                                 context, index, node->intermediates->size,\n+                                 node->intermediates->data, &tensor_index));\n+  *tensor = GetTensorAtIndex(context, tensor_index);\n+  return kTfLiteOk;\n+}\n+#endif  // TF_LITE_STATIC_MEMORY\n+\n // Per-axis\n TfLiteStatus PopulateConvolutionQuantizationParams(\n     TfLiteContext* context, const TfLiteTensor* input,"
    },
    "modified_file_16": {
        "mod_filename": "tensorflow/lite/kernels/kernel_util.h",
        "status": "modified",
        "add_lines": 63,
        "dele_lines": 30,
        "patch": "@@ -40,6 +40,17 @@ namespace tflite {\n const TfLiteTensor* GetInput(const TfLiteContext* context,\n                              const TfLiteNode* node, int index);\n \n+// Same as `GetInput` but returns boolean and uses output argument for tensor.\n+//\n+//   TfLiteTensor* my_tensor;\n+//   TF_LITE_ENSURE_OK(context,\n+//                     GetInputSafe(context, node, kMyTensorIdx, &my_tensor));\n+//   // can use my_tensor directly from here onwards, it is not nullptr\n+//\n+// Should be used in cases where the binary size is too large.\n+TfLiteStatus GetInputSafe(const TfLiteContext* context, const TfLiteNode* node,\n+                          int index, const TfLiteTensor** tensor);\n+\n // Note: You must check if result is not null:\n //\n //   TfLiteTensor* my_tensor = GetVariableInput(context, node, kMyTensorIdx);\n@@ -60,6 +71,17 @@ TfLiteTensor* GetVariableInput(TfLiteContext* context, const TfLiteNode* node,\n TfLiteTensor* GetOutput(TfLiteContext* context, const TfLiteNode* node,\n                         int index);\n \n+// Same as `GetOutput` but returns boolean and uses output argument for tensor.\n+//\n+//   TfLiteTensor* my_tensor;\n+//   TF_LITE_ENSURE_OK(context,\n+//                     GetOutputSafe(context, node, kMyTensorIdx, &my_tensor));\n+//   // can use my_tensor directly from here onwards, it is not nullptr\n+//\n+// Should be used in cases where the binary size is too large.\n+TfLiteStatus GetOutputSafe(const TfLiteContext* context, const TfLiteNode* node,\n+                           int index, TfLiteTensor** tensor);\n+\n // Note: You must check if result is not null:\n //\n //   TfLiteTensor* my_tensor = GetOptionalInputTensor(context, node, kIdx);\n@@ -72,11 +94,6 @@ TfLiteTensor* GetOutput(TfLiteContext* context, const TfLiteNode* node,\n const TfLiteTensor* GetOptionalInputTensor(const TfLiteContext* context,\n                                            const TfLiteNode* node, int index);\n \n-inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }\n-inline int SizeOfDimension(const TfLiteTensor* t, int dim) {\n-  return t->dims->data[dim];\n-}\n-\n #ifndef TF_LITE_STATIC_MEMORY\n // Note: You must check if result is not null:\n //\n@@ -85,18 +102,22 @@ inline int SizeOfDimension(const TfLiteTensor* t, int dim) {\n //\n // This is because the index might point to the optional tensor constant\n // (kTfLiteOptionalTensor) in which case there is no tensor to return.\n-inline TfLiteTensor* GetTemporary(TfLiteContext* context,\n-                                  const TfLiteNode* node, int index) {\n-  if (index >= 0 && index < node->temporaries->size) {\n-    const int tensor_index = node->temporaries->data[index];\n-    if (tensor_index != kTfLiteOptionalTensor) {\n-      if (context->tensors != nullptr) {\n-        return &context->tensors[tensor_index];\n-      }\n-    }\n-  }\n-  return nullptr;\n-}\n+TfLiteTensor* GetTemporary(TfLiteContext* context, const TfLiteNode* node,\n+                           int index);\n+\n+// Same as `GetTemporary` but returns boolean and uses output argument for\n+// tensor.\n+//\n+//   TfLiteTensor* my_tensor;\n+//   TF_LITE_ENSURE_OK(context,\n+//                     GetTemporarySafe(context, node, kMyTensorIdx,\n+//                     &my_tensor));\n+//   // can use my_tensor directly from here onwards, it is not nullptr\n+//\n+// Should be used in cases where the binary size is too large.\n+TfLiteStatus GetTemporarySafe(const TfLiteContext* context,\n+                              const TfLiteNode* node, int index,\n+                              TfLiteTensor** tensor);\n \n // Note: You must check if result is not null:\n //\n@@ -105,25 +126,37 @@ inline TfLiteTensor* GetTemporary(TfLiteContext* context,\n //\n // This is because the index might point to the optional tensor constant\n // (kTfLiteOptionalTensor) in which case there is no tensor to return.\n-inline const TfLiteTensor* GetIntermediates(TfLiteContext* context,\n-                                            const TfLiteNode* node, int index) {\n-  if (index >= 0 && index < node->intermediates->size) {\n-    const int tensor_index = node->intermediates->data[index];\n-    if (tensor_index != kTfLiteOptionalTensor) {\n-      if (context->tensors != nullptr) {\n-        return &context->tensors[tensor_index];\n-      }\n-    }\n-  }\n-  return nullptr;\n+const TfLiteTensor* GetIntermediates(TfLiteContext* context,\n+                                     const TfLiteNode* node, int index);\n+\n+// Same as `GetIntermediates` but returns boolean and uses output argument for\n+// tensor.\n+//\n+//   TfLiteTensor* my_tensor;\n+//   TF_LITE_ENSURE_OK(context,\n+//                     GetIntermediatesSafe(context, node, kMyTensorIdx,\n+//                     &my_tensor));\n+//   // can use my_tensor directly from here onwards, it is not nullptr\n+//\n+// Should be used in cases where the binary size is too large.\n+TfLiteStatus GetIntermediatesSafe(const TfLiteContext* context,\n+                                  const TfLiteNode* node, int index,\n+                                  TfLiteTensor** tensor);\n+#endif  // TF_LITE_STATIC_MEMORY\n+\n+inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }\n+inline int SizeOfDimension(const TfLiteTensor* t, int dim) {\n+  return t->dims->data[dim];\n }\n \n+inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }\n+inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }\n+\n+#ifndef TF_LITE_STATIC_MEMORY\n inline int NumIntermediates(const TfLiteNode* node) {\n   return node->intermediates->size;\n }\n #endif  // TF_LITE_STATIC_MEMORY\n-inline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }\n-inline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }\n \n inline int64_t NumElements(const TfLiteIntArray* dims) {\n   int64_t count = 1;"
    },
    "modified_file_17": {
        "mod_filename": "tensorflow/lite/profiling/profile_summarizer_test.cc",
        "status": "modified",
        "add_lines": 8,
        "dele_lines": 4,
        "patch": "@@ -36,10 +36,14 @@ namespace {\n const char* kOpName = \"SimpleOpEval\";\n \n TfLiteStatus SimpleOpEval(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input1 = tflite::GetInput(context, node, /*index=*/0);\n-  const TfLiteTensor* input2 = tflite::GetInput(context, node, /*index=*/1);\n-\n-  TfLiteTensor* output = GetOutput(context, node, /*index=*/0);\n+  const TfLiteTensor* input1;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, /*index=*/0, &input1));\n+  const TfLiteTensor* input2;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, /*index=*/1, &input2));\n+\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, /*index=*/0, &output));\n \n   int32_t* output_data = output->data.i32;\n   *output_data = *(input1->data.i32) + *(input2->data.i32);"
    },
    "modified_file_18": {
        "mod_filename": "tensorflow/lite/tools/optimize/calibration/builtin_logging_ops/lstm.cc",
        "status": "modified",
        "add_lines": 61,
        "dele_lines": 23,
        "patch": "@@ -466,26 +466,51 @@ TfLiteStatus lstm_eval(TfLiteContext* context, TfLiteNode* node, Logger* logger,\n                        ErrorReporter* error_reporter) {\n   const auto* params = static_cast<TfLiteLSTMParams*>(node->builtin_data);\n \n-  const TfLiteTensor* input =\n-      GetInput(context, node, ops::builtin::lstm::full::kInputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(\n+      context, GetInputSafe(context, node,\n+                            ops::builtin::lstm::full::kInputTensor, &input));\n \n   const TfLiteTensor* input_to_input_weights = GetOptionalInputTensor(\n       context, node, ops::builtin::lstm::full::kInputToInputWeightsTensor);\n-  const TfLiteTensor* input_to_forget_weights = GetInput(\n-      context, node, ops::builtin::lstm::full::kInputToForgetWeightsTensor);\n-  const TfLiteTensor* input_to_cell_weights = GetInput(\n-      context, node, ops::builtin::lstm::full::kInputToCellWeightsTensor);\n-  const TfLiteTensor* input_to_output_weights = GetInput(\n-      context, node, ops::builtin::lstm::full::kInputToOutputWeightsTensor);\n+  const TfLiteTensor* input_to_forget_weights;\n+  TF_LITE_ENSURE_OK(\n+      context,\n+      GetInputSafe(context, node,\n+                   ops::builtin::lstm::full::kInputToForgetWeightsTensor,\n+                   &input_to_forget_weights));\n+  const TfLiteTensor* input_to_cell_weights;\n+  TF_LITE_ENSURE_OK(\n+      context, GetInputSafe(context, node,\n+                            ops::builtin::lstm::full::kInputToCellWeightsTensor,\n+                            &input_to_cell_weights));\n+  const TfLiteTensor* input_to_output_weights;\n+  TF_LITE_ENSURE_OK(\n+      context,\n+      GetInputSafe(context, node,\n+                   ops::builtin::lstm::full::kInputToOutputWeightsTensor,\n+                   &input_to_output_weights));\n \n   const TfLiteTensor* recurrent_to_input_weights = GetOptionalInputTensor(\n       context, node, ops::builtin::lstm::full::kRecurrentToInputWeightsTensor);\n-  const TfLiteTensor* recurrent_to_forget_weights = GetInput(\n-      context, node, ops::builtin::lstm::full::kRecurrentToForgetWeightsTensor);\n-  const TfLiteTensor* recurrent_to_cell_weights = GetInput(\n-      context, node, ops::builtin::lstm::full::kRecurrentToCellWeightsTensor);\n-  const TfLiteTensor* recurrent_to_output_weights = GetInput(\n-      context, node, ops::builtin::lstm::full::kRecurrentToOutputWeightsTensor);\n+  const TfLiteTensor* recurrent_to_forget_weights;\n+  TF_LITE_ENSURE_OK(\n+      context,\n+      GetInputSafe(context, node,\n+                   ops::builtin::lstm::full::kRecurrentToForgetWeightsTensor,\n+                   &recurrent_to_forget_weights));\n+  const TfLiteTensor* recurrent_to_cell_weights;\n+  TF_LITE_ENSURE_OK(\n+      context,\n+      GetInputSafe(context, node,\n+                   ops::builtin::lstm::full::kRecurrentToCellWeightsTensor,\n+                   &recurrent_to_cell_weights));\n+  const TfLiteTensor* recurrent_to_output_weights;\n+  TF_LITE_ENSURE_OK(\n+      context,\n+      GetInputSafe(context, node,\n+                   ops::builtin::lstm::full::kRecurrentToOutputWeightsTensor,\n+                   &recurrent_to_output_weights));\n \n   const TfLiteTensor* cell_to_input_weights = GetOptionalInputTensor(\n       context, node, ops::builtin::lstm::full::kCellToInputWeightsTensor);\n@@ -509,20 +534,31 @@ TfLiteStatus lstm_eval(TfLiteContext* context, TfLiteNode* node, Logger* logger,\n \n   const TfLiteTensor* input_gate_bias = GetOptionalInputTensor(\n       context, node, ops::builtin::lstm::full::kInputGateBiasTensor);\n-  const TfLiteTensor* forget_gate_bias =\n-      GetInput(context, node, ops::builtin::lstm::full::kForgetGateBiasTensor);\n-  const TfLiteTensor* cell_gate_bias =\n-      GetInput(context, node, ops::builtin::lstm::full::kCellGateBiasTensor);\n-  const TfLiteTensor* output_gate_bias =\n-      GetInput(context, node, ops::builtin::lstm::full::kOutputGateBiasTensor);\n+  const TfLiteTensor* forget_gate_bias;\n+  TF_LITE_ENSURE_OK(\n+      context, GetInputSafe(context, node,\n+                            ops::builtin::lstm::full::kForgetGateBiasTensor,\n+                            &forget_gate_bias));\n+  const TfLiteTensor* cell_gate_bias;\n+  TF_LITE_ENSURE_OK(\n+      context,\n+      GetInputSafe(context, node, ops::builtin::lstm::full::kCellGateBiasTensor,\n+                   &cell_gate_bias));\n+  const TfLiteTensor* output_gate_bias;\n+  TF_LITE_ENSURE_OK(\n+      context, GetInputSafe(context, node,\n+                            ops::builtin::lstm::full::kOutputGateBiasTensor,\n+                            &output_gate_bias));\n \n   const TfLiteTensor* projection_weights = GetOptionalInputTensor(\n       context, node, ops::builtin::lstm::full::kProjectionWeightsTensor);\n   const TfLiteTensor* projection_bias = GetOptionalInputTensor(\n       context, node, ops::builtin::lstm::full::kProjectionBiasTensor);\n \n   // Index the scratch buffers pointers to the global scratch buffer.\n-  TfLiteTensor* scratch_buffer = GetTemporary(context, node, /*index=*/0);\n+  TfLiteTensor* scratch_buffer;\n+  TF_LITE_ENSURE_OK(\n+      context, GetTemporarySafe(context, node, /*index=*/0, &scratch_buffer));\n \n   TfLiteTensor* output_state = GetVariableInput(\n       context, node, ops::builtin::lstm::full::kOutputStateTensor);\n@@ -531,8 +567,10 @@ TfLiteStatus lstm_eval(TfLiteContext* context, TfLiteNode* node, Logger* logger,\n       context, node, ops::builtin::lstm::full::kCellStateTensor);\n   TF_LITE_ENSURE(context, cell_state != nullptr);\n \n-  TfLiteTensor* output =\n-      GetOutput(context, node, ops::builtin::lstm::full::kOutputTensor);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(\n+      context, GetOutputSafe(context, node,\n+                             ops::builtin::lstm::full::kOutputTensor, &output));\n \n   std::vector<int> intermediate_tensor_indexes(node->intermediates->size);\n   for (int i = 0; i < node->intermediates->size; ++i) {"
    }
}