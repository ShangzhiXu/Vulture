{
    "current_hash": "fff2c8326280c07733828f990548979bdc893859",
    "parent_hash": "e84a6501eb9d4c5289065f007801c483ab8f3e8c",
    "modified_file_0": {
        "mod_filename": "tensorflow/lite/micro/kernels/activations.cc",
        "status": "modified",
        "add_lines": 3,
        "dele_lines": 0,
        "patch": "@@ -139,7 +139,9 @@ TfLiteStatus ReluPrepare(TfLiteContext* context, TfLiteNode* node) {\n   ReluOpData* data = static_cast<ReluOpData*>(node->user_data);\n \n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   if (input->type == kTfLiteInt8) {\n     CalculateReluOpData<int8_t>(input, output, data);\n@@ -200,6 +202,7 @@ TfLiteStatus Relu6Prepare(TfLiteContext* context, TfLiteNode* node) {\n   Relu6OpData* data = static_cast<Relu6OpData*>(node->user_data);\n \n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n \n   if (input->type == kTfLiteInt8) {\n     data->six_int8 = FloatToAsymmetricQuantizedInt8(6.0f, input->params.scale,"
    },
    "modified_file_1": {
        "mod_filename": "tensorflow/lite/micro/kernels/add.cc",
        "status": "modified",
        "add_lines": 3,
        "dele_lines": 0,
        "patch": "@@ -201,8 +201,11 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TFLITE_DCHECK(node->builtin_data != nullptr);\n \n   const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);\n+  TF_LITE_ENSURE(context, input1 != nullptr);\n   const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);\n+  TF_LITE_ENSURE(context, input2 != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   OpData* data = static_cast<OpData*>(node->user_data);\n   auto* params = reinterpret_cast<TfLiteAddParams*>(node->builtin_data);"
    },
    "modified_file_2": {
        "mod_filename": "tensorflow/lite/micro/kernels/ceil.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 0,
        "patch": "@@ -30,7 +30,9 @@ constexpr int kOutputTensor = 0;\n \n TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, kTfLiteFloat32);"
    },
    "modified_file_3": {
        "mod_filename": "tensorflow/lite/micro/kernels/circular_buffer.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 0,
        "patch": "@@ -77,7 +77,9 @@ void Free(TfLiteContext* context, void* buffer) { op_data_counter = 0; }\n \n TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   TF_LITE_ENSURE(context, input != nullptr);\n   TF_LITE_ENSURE(context, output != nullptr);"
    },
    "modified_file_4": {
        "mod_filename": "tensorflow/lite/micro/kernels/comparisons.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 0,
        "patch": "@@ -619,7 +619,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   OpData* data = static_cast<OpData*>(node->user_data);\n \n   const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);\n+  TF_LITE_ENSURE(context, input1 != nullptr);\n   const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);\n+  TF_LITE_ENSURE(context, input2 != nullptr);\n \n   if (input1->type == kTfLiteUInt8 || input1->type == kTfLiteInt8) {\n     auto input1_offset = -input1->params.zero_point;"
    },
    "modified_file_5": {
        "mod_filename": "tensorflow/lite/micro/kernels/concatenation.cc",
        "status": "modified",
        "add_lines": 12,
        "dele_lines": 3,
        "patch": "@@ -136,8 +136,12 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   const TfLiteConcatenationParams* params =\n       reinterpret_cast<TfLiteConcatenationParams*>(node->builtin_data);\n \n-  TfLiteType input_type = GetInput(context, node, 0)->type;\n-  TfLiteType output_type = GetOutput(context, node, kOutputTensor)->type;\n+  const TfLiteTensor* input_tensor = GetInput(context, node, 0);\n+  TF_LITE_ENSURE(context, input_tensor != nullptr);\n+  TfLiteType input_type = input_tensor->type;\n+  const TfLiteTensor* output_tensor = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output_tensor != nullptr);\n+  TfLiteType output_type = output_tensor->type;\n \n   // Check activation and input type\n   TF_LITE_ENSURE_EQ(context, params->activation, kTfLiteActNone);\n@@ -156,6 +160,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   // Shapes with dimensions >4 are not yet supported with static allocation.\n   for (int i = 0; i < num_inputs; ++i) {\n     const TfLiteTensor* input = GetInput(context, node, i);\n+    TF_LITE_ENSURE(context, input != nullptr);\n     int num_dimensions = NumDimensions(input);\n \n     if (num_dimensions > 4) {\n@@ -173,6 +178,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   OpData* data = static_cast<OpData*>(node->user_data);\n \n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   switch (output_type) {  // Already know in/outtypes are same.\n     case kTfLiteFloat32:\n@@ -199,6 +205,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n       // Store input scale and zero point values in OpParams:\n       for (int i = 0; i < node->inputs->size; ++i) {\n         const TfLiteTensor* t = GetInput(context, node, i);\n+        TF_LITE_ENSURE(context, t != nullptr);\n         input_scales[i] = t->params.scale;\n         input_zero_points[i] = t->params.zero_point;\n       }\n@@ -220,7 +227,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n-  TfLiteType output_type = GetOutput(context, node, kOutputTensor)->type;\n+  const TfLiteTensor* output_tensor = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output_tensor != nullptr);\n+  TfLiteType output_type = output_tensor->type;\n \n   switch (output_type) {  // Already know in/outtypes are same.\n     case kTfLiteFloat32:"
    },
    "modified_file_6": {
        "mod_filename": "tensorflow/lite/micro/kernels/conv.cc",
        "status": "modified",
        "add_lines": 6,
        "dele_lines": 0,
        "patch": "@@ -97,10 +97,13 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,\n   // parameters set. This is usually done during quantized training.\n   if (data_type != kTfLiteFloat32) {\n     const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+    TF_LITE_ENSURE(context, input != nullptr);\n     const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);\n+    TF_LITE_ENSURE(context, filter != nullptr);\n     const TfLiteTensor* bias =\n         GetOptionalInputTensor(context, node, kBiasTensor);\n     TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+    TF_LITE_ENSURE(context, output != nullptr);\n     int output_channels = filter->dims->data[kConvQuantizedDimension];\n \n     TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(\n@@ -127,8 +130,11 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   const auto params = static_cast<const TfLiteConvParams*>(node->builtin_data);\n \n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);\n+  TF_LITE_ENSURE(context, filter != nullptr);\n \n   int input_width = input->dims->data[2];\n   int input_height = input->dims->data[1];"
    },
    "modified_file_7": {
        "mod_filename": "tensorflow/lite/micro/kernels/depthwise_conv.cc",
        "status": "modified",
        "add_lines": 6,
        "dele_lines": 0,
        "patch": "@@ -82,10 +82,13 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,\n   // parameters set. This is usually done during quantized training.\n   if (data_type != kTfLiteFloat32) {\n     const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+    TF_LITE_ENSURE(context, input != nullptr);\n     const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);\n+    TF_LITE_ENSURE(context, filter != nullptr);\n     const TfLiteTensor* bias =\n         GetOptionalInputTensor(context, node, kBiasTensor);\n     TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+    TF_LITE_ENSURE(context, output != nullptr);\n     int num_channels = filter->dims->data[kDepthwiseConvQuantizedDimension];\n \n     return tflite::PopulateConvolutionQuantizationParams(\n@@ -114,8 +117,11 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   OpData* data = static_cast<OpData*>(node->user_data);\n \n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);\n+  TF_LITE_ENSURE(context, filter != nullptr);\n \n   const TfLiteType data_type = input->type;\n   int width = SizeOfDimension(input, 2);"
    },
    "modified_file_8": {
        "mod_filename": "tensorflow/lite/micro/kernels/dequantize.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 0,
        "patch": "@@ -52,7 +52,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n \n   // TODO(b/140515557): Add cached dequant to improve hybrid model performance.\n   const TfLiteTensor* input = GetInput(context, node, 0);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, 0);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   TF_LITE_ENSURE(context, input->type == kTfLiteUInt8 ||\n                               input->type == kTfLiteInt8 ||"
    },
    "modified_file_9": {
        "mod_filename": "tensorflow/lite/micro/kernels/elementwise.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 0,
        "patch": "@@ -41,7 +41,9 @@ TfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n   const TfLiteTensor* input = GetInput(context, node, 0);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, 0);\n+  TF_LITE_ENSURE(context, output != nullptr);\n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n   if (!IsSupportedType(input->type)) {\n     TF_LITE_KERNEL_LOG(context, \"Input data type %s (%d) is not supported.\","
    },
    "modified_file_10": {
        "mod_filename": "tensorflow/lite/micro/kernels/fully_connected.cc",
        "status": "modified",
        "add_lines": 3,
        "dele_lines": 0,
        "patch": "@@ -93,9 +93,12 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n       static_cast<const TfLiteFullyConnectedParams*>(node->builtin_data);\n \n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   const TfLiteTensor* filter = GetInput(context, node, kWeightsTensor);\n+  TF_LITE_ENSURE(context, filter != nullptr);\n   const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n   TF_LITE_ENSURE_MSG(context, input->type == filter->type,"
    },
    "modified_file_11": {
        "mod_filename": "tensorflow/lite/micro/kernels/hard_swish.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 0,
        "patch": "@@ -45,7 +45,9 @@ TfLiteStatus HardSwishPrepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n \n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8) {\n     HardSwishParams* params = static_cast<HardSwishParams*>(node->user_data);"
    },
    "modified_file_12": {
        "mod_filename": "tensorflow/lite/micro/kernels/l2norm.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 0,
        "patch": "@@ -50,7 +50,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n \n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   TF_LITE_ENSURE(context, NumDimensions(input) <= 4);\n "
    },
    "modified_file_13": {
        "mod_filename": "tensorflow/lite/micro/kernels/logistic.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 0,
        "patch": "@@ -43,7 +43,9 @@ struct OpData {\n TfLiteStatus CalculateArithmeticOpData(TfLiteContext* context, TfLiteNode* node,\n                                        OpData* data) {\n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n   if (input->type == kTfLiteInt8) {"
    },
    "modified_file_14": {
        "mod_filename": "tensorflow/lite/micro/kernels/mul.cc",
        "status": "modified",
        "add_lines": 3,
        "dele_lines": 0,
        "patch": "@@ -51,8 +51,11 @@ struct OpData {\n TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,\n                              TfLiteMulParams* params, OpData* data) {\n   const TfLiteTensor* input1 = GetInput(context, node, kInput1Tensor);\n+  TF_LITE_ENSURE(context, input1 != nullptr);\n   const TfLiteTensor* input2 = GetInput(context, node, kInput2Tensor);\n+  TF_LITE_ENSURE(context, input2 != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);"
    },
    "modified_file_15": {
        "mod_filename": "tensorflow/lite/micro/kernels/pad.cc",
        "status": "modified",
        "add_lines": 3,
        "dele_lines": 0,
        "patch": "@@ -50,10 +50,13 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n \n   const TfLiteTensor* input = GetInput(context, node, /*index=*/0);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   const TfLiteTensor* paddings = GetInput(context, node, /*index=*/1);\n+  TF_LITE_ENSURE(context, paddings != nullptr);\n   const TfLiteTensor* constant_values =\n       NumInputs(node) == 3 ? GetInput(context, node, /*index=*/2) : nullptr;\n   TfLiteTensor* output = GetOutput(context, node, /*index=*/0);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   TF_LITE_ENSURE_EQ(context, input->type, output->type);\n "
    },
    "modified_file_16": {
        "mod_filename": "tensorflow/lite/micro/kernels/pooling.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 0,
        "patch": "@@ -222,7 +222,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   OpData* data = static_cast<OpData*>(node->user_data);\n \n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, data));\n "
    },
    "modified_file_17": {
        "mod_filename": "tensorflow/lite/micro/kernels/prelu.cc",
        "status": "modified",
        "add_lines": 3,
        "dele_lines": 0,
        "patch": "@@ -95,8 +95,11 @@ TfLiteStatus PreluPrepare(TfLiteContext* context, TfLiteNode* node) {\n   PreluParams* params = static_cast<PreluParams*>(node->user_data);\n \n   const TfLiteTensor* input = GetInput(context, node, 0);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   const TfLiteTensor* alpha = GetInput(context, node, 1);\n+  TF_LITE_ENSURE(context, alpha != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, 0);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   return CalculatePreluParams(input, alpha, output, params);\n }"
    },
    "modified_file_18": {
        "mod_filename": "tensorflow/lite/micro/kernels/quantize.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 0,
        "patch": "@@ -50,7 +50,9 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n \n   const TfLiteTensor* input = GetInput(context, node, 0);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, 0);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   // TODO(b/128934713): Add support for fixed-point per-channel quantization.\n   // Currently this only support affine per-layer quantization."
    },
    "modified_file_19": {
        "mod_filename": "tensorflow/lite/micro/kernels/reduce.cc",
        "status": "modified",
        "add_lines": 1,
        "dele_lines": 0,
        "patch": "@@ -64,6 +64,7 @@ TfLiteStatus PrepareSimple(TfLiteContext* context, TfLiteNode* node) {\n \n   // Validate axis type\n   const TfLiteTensor* axis = GetInput(context, node, 1);\n+  TF_LITE_ENSURE(context, axis != nullptr);\n   TF_LITE_ENSURE_TYPES_EQ(context, axis->type, kTfLiteInt32);\n \n   if (input->type == kTfLiteInt8) {"
    },
    "modified_file_20": {
        "mod_filename": "tensorflow/lite/micro/kernels/reshape.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 0,
        "patch": "@@ -32,7 +32,9 @@ constexpr int kOutputTensor = 0;\n \n TfLiteStatus ReshapeOutput(TfLiteContext* context, TfLiteNode* node) {\n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n   // Tensorflow's Reshape allows one of the shape components to have the\n   // special -1 value, meaning it will be calculated automatically based on the\n   // input. Here we calculate what that dimension should be so that the number"
    },
    "modified_file_21": {
        "mod_filename": "tensorflow/lite/micro/kernels/round.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 0,
        "patch": "@@ -30,7 +30,9 @@ constexpr int kOutputTensor = 0;\n \n TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, kTfLiteFloat32);"
    },
    "modified_file_22": {
        "mod_filename": "tensorflow/lite/micro/kernels/softmax.cc",
        "status": "modified",
        "add_lines": 2,
        "dele_lines": 0,
        "patch": "@@ -119,9 +119,11 @@ TfLiteStatus SoftmaxPrepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n   const TfLiteTensor* input = GetInput(context, node, 0);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TF_LITE_ENSURE(context, NumDimensions(input) >= 1);\n \n   TfLiteTensor* output = GetOutput(context, node, 0);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   TFLITE_DCHECK(node->user_data != nullptr);\n   SoftmaxParams* data = static_cast<SoftmaxParams*>(node->user_data);"
    },
    "modified_file_23": {
        "mod_filename": "tensorflow/lite/micro/kernels/split.cc",
        "status": "modified",
        "add_lines": 1,
        "dele_lines": 0,
        "patch": "@@ -69,6 +69,7 @@ TfLiteStatus SplitImpl(TfLiteContext* context, TfLiteNode* node,\n \n TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   const TfLiteTensor* axis = GetInput(context, node, 0);\n+  TF_LITE_ENSURE(context, axis != nullptr);\n \n   // Dynamic output tensors are needed if axis tensor is not constant.\n   // But Micro doesn't support dynamic memory allocation, so we only support"
    },
    "modified_file_24": {
        "mod_filename": "tensorflow/lite/micro/kernels/sub.cc",
        "status": "modified",
        "add_lines": 3,
        "dele_lines": 0,
        "patch": "@@ -108,8 +108,11 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   auto* params = reinterpret_cast<TfLiteSubParams*>(node->builtin_data);\n \n   const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);\n+  TF_LITE_ENSURE(context, input1 != nullptr);\n   const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);\n+  TF_LITE_ENSURE(context, input2 != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   TF_LITE_ENSURE_STATUS(\n       CalculateOpData(context, params, input1, input2, output, data));"
    },
    "modified_file_25": {
        "mod_filename": "tensorflow/lite/micro/kernels/svdf.cc",
        "status": "modified",
        "add_lines": 5,
        "dele_lines": 0,
        "patch": "@@ -366,13 +366,17 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   // [4] = Activation State (variable),\n   //         {2, batch_size, memory_size * num_filters}\n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   const TfLiteTensor* weights_feature =\n       GetInput(context, node, kWeightsFeatureTensor);\n+  TF_LITE_ENSURE(context, weights_feature != nullptr);\n   const TfLiteTensor* weights_time =\n       GetInput(context, node, kWeightsTimeTensor);\n+  TF_LITE_ENSURE(context, weights_time != nullptr);\n   const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);\n   const TfLiteTensor* activation_state =\n       GetInput(context, node, kInputActivationStateTensor);\n+  TF_LITE_ENSURE(context, activation_state != nullptr);\n \n   // Define input constants based on input tensor definition above:\n   const int rank = params->rank;\n@@ -392,6 +396,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   // [0] = float/int8_t, {2, batch_size, num_units}\n   TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n   TF_LITE_ENSURE_EQ(context, NumDimensions(output), 2);\n   TF_LITE_ENSURE_EQ(context, output->dims->data[0], batch_size);\n   TF_LITE_ENSURE_EQ(context, output->dims->data[1], num_units);"
    },
    "modified_file_26": {
        "mod_filename": "tensorflow/lite/micro/kernels/tanh.cc",
        "status": "modified",
        "add_lines": 3,
        "dele_lines": 0,
        "patch": "@@ -51,7 +51,9 @@ TfLiteStatus CalculateArithmeticOpData(TfLiteContext* context, TfLiteNode* node,\n   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n+  TF_LITE_ENSURE(context, output != nullptr);\n \n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n \n@@ -76,6 +78,7 @@ TfLiteStatus TanhPrepare(TfLiteContext* context, TfLiteNode* node) {\n   OpData* data = static_cast<OpData*>(node->user_data);\n \n   const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  TF_LITE_ENSURE(context, input != nullptr);\n   data->input_zero_point = input->params.zero_point;\n   return CalculateArithmeticOpData(context, node, data);\n }"
    }
}